{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9c32af-40f5-42b9-837d-a0ef3200b33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy.io as sio\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from logging import getLogger\n",
    "from ast import Global\n",
    "from functools import reduce\n",
    "from trd import *\n",
    "from ALDI import *\n",
    "from utils import *\n",
    "sys.path.append('/root/linghui/drs/TRD-main/daisyRec/')\n",
    "import daisy\n",
    "from daisy.utils.loader import RawDataReader, Preprocessor\n",
    "from daisy.utils.splitter import TestSplitter, ValidationSplitter\n",
    "from daisy.utils.config import init_seed, init_config, init_logger\n",
    "from daisy.utils.metrics import MAP, NDCG, Recall, Precision, HR, MRR\n",
    "from daisy.utils.sampler import BasicNegtiveSampler, SkipGramNegativeSampler, UniqueNegativeSampler\n",
    "from daisy.utils.dataset import AEDataset, BasicDataset, CandidatesDataset, get_dataloader\n",
    "from daisy.utils.utils import get_history_matrix, get_ur, build_candidates_set, ensure_dir, get_inter_matrix\n",
    "from daisy.model.MFRecommender import MF\n",
    "from daisy.model.FMRecommender import FM\n",
    "from daisy.model.NFMRecommender import NFM\n",
    "from daisy.model.NGCFRecommender import NGCF\n",
    "from daisy.model.EASERecommender import EASE\n",
    "from daisy.model.SLiMRecommender import SLiM\n",
    "from daisy.model.VAECFRecommender import VAECF\n",
    "from daisy.model.NeuMFRecommender import NeuMF\n",
    "from daisy.model.PopRecommender import MostPop\n",
    "from daisy.model.KNNCFRecommender import ItemKNNCF\n",
    "from daisy.model.PureSVDRecommender import PureSVD\n",
    "from daisy.model.Item2VecRecommender import Item2Vec\n",
    "from daisy.model.LightGCNRecommender import LightGCN\n",
    "from daisy.utils.metrics import calc_ranking_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1740a01d-e194-4975-b8d8-e65763b8fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23 Jan 09:57 INFO - {'version': 'daisy_1/', 'trd_version': 'drs_trd', 'gpu': '1', 'seed': 2024, 'reproducibility': True, 'state': None, 'optimization_metric': 'ndcg', 'hyperopt_trail': 20, 'tune_testset': False, 'tune_pack': '{\"lr\": [0.001, 0.005, 0.01], \"reg_1\": [0.000001, 0.00001, 0.0001, 0.001, 0], \"reg_2\": [0.000001, 0.00001, 0.0001, 0.001, 0], \"num_layers\": [2, 3, 4]}', 'algo_name': 'mf', 'data_path': '/data/linghui/', 'save_path': '/data/linghui/drs/', 'res_path': None, 'dataset': 'steam', 'val_method': 'tsbr', 'test_method': 'tsbr', 'fold_num': 1, 'val_size': 0.1, 'test_size': 0.2, 'topk': 50, 'n_actions': 20, 'cand_num': 1000, 'sample_method': 'uniform', 'sample_ratio': 0, 'num_ng': 4, 'batch_size': 1024, 'loss_type': 'BPR', 'init_method': 'default', 'optimizer': 'default', 'early_stop': True, 'content_dim': 328, 'train_step': 120, 'prepro': '10filter', 'level': 'u', 'positive_threshold': 1.0, 'UID_NAME': 'user', 'IID_NAME': 'item', 'INTER_NAME': 'label', 'TID_NAME': 'timestamp', 'binary_inter': True, 'metrics': ['recall', 'mrr', 'ndcg', 'hit', 'precision'], 'train_base_ratio': 0.3, 'train_rl_ratio': 0.2, 'test_stage': 5, 'fine_tune': 'no', 'debias_method': 'fair', 'weight': 0.01, 'add_reg': 'yes', 'burnin': 'no', 'save': 0, 'alpha': 2, 'beta': 0.5, 'gama': 0.1, 'max_epoch': 20, 'freq_coef_M': 4, 'tws': 1, 'use_aldi': 1, 'factors': 100, 'epochs': 20, 'lr': 0.01, 'reg_1': 1e-06, 'reg_2': 1e-06}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user number: 79216  item number: 14258\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    'mostpop': MostPop,\n",
    "    'slim': SLiM,\n",
    "    'itemknn': ItemKNNCF,\n",
    "    'puresvd': PureSVD,\n",
    "    'mf': MF,\n",
    "    'fm': FM,\n",
    "    'ngcf': NGCF,\n",
    "    'neumf': NeuMF,\n",
    "    'nfm': NFM,\n",
    "    'multi-vae': VAECF,\n",
    "    'item2vec': Item2Vec,\n",
    "    'ease': EASE,\n",
    "    'lightgcn': LightGCN,\n",
    "}\n",
    "\n",
    "\n",
    "config = init_config()\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "init_logger(config)\n",
    "logger = getLogger()\n",
    "logger.info(config)\n",
    "config['logger'] = logger\n",
    "\n",
    "trd_version = 'new_flag_reg'\n",
    "# trd_version = 'efficient_v1'\n",
    "save_path = config['save_path'] + config['version']\n",
    "ensure_dir(save_path)\n",
    "\n",
    "file_path = save_path + f'{config[\"dataset\"]}/'\n",
    "ensure_dir(file_path)\n",
    "\n",
    "saved_data_path = config['save_path'] + f'daisy_1/{config[\"dataset\"]}/' + 'data/'\n",
    "ensure_dir(saved_data_path)\n",
    "\n",
    "saved_result_path = file_path + f'{config[\"algo_name\"]}/'\n",
    "ensure_dir(saved_result_path)\n",
    "\n",
    "saved_model_path = saved_result_path + 'model/'\n",
    "ensure_dir(saved_model_path)\n",
    "\n",
    "saved_trd_path = saved_result_path + f'{trd_version}/'\n",
    "ensure_dir(saved_trd_path)\n",
    "\n",
    "saved_rec_path = saved_result_path + 'rec_list/'\n",
    "ensure_dir(saved_rec_path)\n",
    "\n",
    "saved_metric_path = saved_result_path + 'metric/'\n",
    "ensure_dir(saved_metric_path)\n",
    "config['res_path'] = saved_metric_path\n",
    "\n",
    "ui_num = np.load(saved_data_path + 'ui_cate.npy')\n",
    "config['user_num'] = ui_num[0]\n",
    "config['item_num'] = ui_num[1]\n",
    "# config['cate_num'] = ui_num[2]\n",
    "print(f\"user number: {config['user_num']}  item number: {config['item_num']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ffc5716-775c-489f-bfe6-62a3bb4f14f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23 Jan 09:57 INFO - begin test stage 0\n",
      "23 Jan 09:57 INFO - :finished update warm/cold_list: 0.075160 s\n",
      "23 Jan 09:57 INFO - begin test stage 1\n",
      "23 Jan 09:57 INFO - :finished update warm/cold_list: 0.129882 s\n",
      "23 Jan 09:57 INFO - begin test stage 2\n",
      "23 Jan 09:57 INFO - :finished update warm/cold_list: 0.110933 s\n",
      "23 Jan 09:57 INFO - begin test stage 3\n",
      "23 Jan 09:57 INFO - :finished update warm/cold_list: 0.135777 s\n",
      "23 Jan 09:57 INFO - begin test stage 4\n",
      "23 Jan 09:57 INFO - :finished update warm/cold_list: 0.150264 s\n"
     ]
    }
   ],
   "source": [
    "train_total_set = pd.read_csv(saved_data_path + 'train_total_set.csv', index_col=0)\n",
    "test_u_all = np.load(saved_data_path + 'test_u_all.npy', allow_pickle=True)\n",
    "test_ucands_all = np.load(saved_data_path + 'test_ucands_all.npy',allow_pickle=True)\n",
    "test_ur_all = np.load(saved_data_path + 'test_ur_all.npy', allow_pickle=True)\n",
    "data_last_stage = train_total_set\n",
    "# result_df_all = pd.DataFrame()\n",
    "warm_item_list_stage = []\n",
    "cold_item_list_stage = []\n",
    "new_item_list_stage = []\n",
    "for stage in range(config['test_stage']):\n",
    "    logger.info(f'begin test stage {stage}')\n",
    "    # load test data\n",
    "    test_u = test_u_all[stage]\n",
    "    test_ur = test_ur_all[stage]\n",
    "    test_ucands = test_ucands_all[stage]\n",
    "    test_set = pd.read_csv(saved_data_path + f'test_set_{stage}.csv')\n",
    "    # test_pd = pd.read_csv(saved_data_path + f'test_df_{stage}', index_col = 0)\n",
    "\n",
    "    # update warm ，cold set and new items in this stage\n",
    "    start_time = time.time()\n",
    "    warm_item_list, cold_item_list = update_new_item(data_last_stage, config)\n",
    "    warm_item_list_stage.append(warm_item_list)\n",
    "    cold_item_list_stage.append(cold_item_list)\n",
    "    # config['warm_item_list'] = warm_item_list\n",
    "    # config['cold_item_list'] = cold_item_list\n",
    "    new_item_list = list(set(test_set[config['IID_NAME']].unique()) - set(warm_item_list))\n",
    "    new_item_list_stage.append(new_item_list)\n",
    "    config['topk_list'] = [10, 20, 50]\n",
    "    data_last_stage = pd.concat([data_last_stage, test_set])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logger.info(f\":finished update warm/cold_list: {elapsed_time:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfe0872-f4a1-4e51-b1ee-f019773844c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_109519/427283569.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_data_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'warm_item_list_stage.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_item_list_stage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_data_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'cold_item_list_stage.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcold_item_list_stage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_data_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'new_item_list_stage.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_item_list_stage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m    523\u001b[0m                            pickle_kwargs=dict(fix_imports=fix_imports))\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "np.save(saved_data_path + 'warm_item_list_stage.npy', warm_item_list_stage)\n",
    "np.save(saved_data_path + 'cold_item_list_stage.npy', cold_item_list_stage)\n",
    "np.save(saved_data_path + 'new_item_list_stage.npy', new_item_list_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b3a2ef-2bcd-467d-9d81-92f810b8fb61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_values_by_id(interaction_history, item_num):\n",
    "    \"\"\"\n",
    "    将用户的交互历史（列表）转换为一个固定长度的列表，交互过的物品位置为 1，未交互过的为 0。\n",
    "    \"\"\"\n",
    "    # 使用 numpy 的向量化操作替代循环\n",
    "    return np.isin(np.arange(item_num), interaction_history).astype(int).tolist()\n",
    "\n",
    "def get_weight(warm_item_list, cold_item_list):\n",
    "    \"\"\"\n",
    "    计算暖项目和冷项目的权重\n",
    "    \"\"\"\n",
    "    warm_num = len(warm_item_list)\n",
    "    warm_weight = np.arange(warm_num, 0, -1)  # 从 warm_num 到 1\n",
    "\n",
    "    cold_num = len(cold_item_list)\n",
    "    if cold_num > 1:\n",
    "        cold_weight = 1 + np.arange(cold_num) * (warm_num - 1) / (cold_num - 1)\n",
    "    else:\n",
    "        cold_weight = np.array([1])\n",
    "    return warm_weight, cold_weight\n",
    "\n",
    "def get_user_hist_tgf(exp_list, warm_item_list, cold_item_list, warm_weight, cold_weight):\n",
    "    \"\"\"\n",
    "    计算用户的分类权重差异（tgf）\n",
    "    \"\"\"\n",
    "    if np.sum(exp_list) == 0:\n",
    "        return 0\n",
    "\n",
    "    # 归一化经验值\n",
    "    exp_list = exp_list / np.sum(exp_list)\n",
    "\n",
    "    # 提取暖项目和冷项目的经验值\n",
    "    warm_exp_list = exp_list[warm_item_list]\n",
    "    cold_exp_list = exp_list[cold_item_list]\n",
    "\n",
    "    # 计算暖项目和冷项目的部分\n",
    "    warm_part = np.sum(warm_exp_list * warm_weight) / len(warm_item_list)\n",
    "    cold_part = np.sum(cold_exp_list * cold_weight) / len(cold_item_list)\n",
    "\n",
    "    # 计算分类的权重差异\n",
    "    user_tgf = warm_part - cold_part\n",
    "    if user_tgf < 0:\n",
    "        user_tgf = user_tgf / (len(warm_item_list) / len(cold_item_list))\n",
    "    return user_tgf\n",
    "\n",
    "def calculate_nc(input_set, target_set):\n",
    "    \"\"\"\n",
    "    统计给定集合中有多少比例的值出现在目标集合中\n",
    "\n",
    "    参数:\n",
    "    input_set: 输入的集合\n",
    "    target_set: 目标集合\n",
    "\n",
    "    返回:\n",
    "    proportion: 出现在目标集合中的比例\n",
    "    \"\"\"\n",
    "    if not input_set:\n",
    "        return 0.0\n",
    "    return len(set(input_set) & set(target_set)) / len(input_set)\n",
    "\n",
    "def process_user_interaction_data(train_rl_set, warm_item_list, cold_item_list, config):\n",
    "    \"\"\"\n",
    "    处理用户交互数据，生成包含用户历史、经验列表、分类权重差异和冷项目比例的 DataFrame。\n",
    "\n",
    "    参数:\n",
    "    - train_rl_set: 训练集的用户交互数据（DataFrame）\n",
    "    - warm_item_list: 暖项目列表\n",
    "    - cold_item_list: 冷项目列表\n",
    "    - cold_item_set: 冷项目集合\n",
    "    - config: 配置字典，包含 item_num 等参数\n",
    "\n",
    "    返回:\n",
    "    - user_rl_df: 处理后的用户交互数据（DataFrame）\n",
    "    \"\"\"\n",
    "    # 按用户分组并生成用户历史列表\n",
    "    user_rl_df = train_rl_set.groupby('user')['item'].apply(list).reset_index(name='user_history')\n",
    "\n",
    "    # 生成用户的经验列表\n",
    "    item_range = np.arange(config['item_num'])\n",
    "    user_rl_df['exp_list'] = user_rl_df['user_history'].apply(\n",
    "        lambda x: np.isin(item_range, x).astype(int).tolist()\n",
    "    )\n",
    "\n",
    "    # 计算暖项目和冷项目的权重\n",
    "    warm_weight, cold_weight = get_weight(warm_item_list, cold_item_list)\n",
    "\n",
    "    # 计算用户的分类权重差异（tgf）\n",
    "    user_rl_df['hist_tgf'] = user_rl_df['exp_list'].apply(\n",
    "        lambda x: get_user_hist_tgf(np.array(x), warm_item_list, cold_item_list, warm_weight, cold_weight)\n",
    "    )\n",
    "\n",
    "    # 计算用户的冷项目比例（hist_nc）\n",
    "    user_rl_df['hist_nc'] = user_rl_df['user_history'].apply(\n",
    "        lambda x: calculate_nc(x, cold_item_list)\n",
    "    )\n",
    "\n",
    "    return user_rl_df\n",
    "\n",
    "def get_common_users(dataframes):\n",
    "    \"\"\"\n",
    "    从多个 Pandas DataFrame 中提取所有共有的 'user' 列值。\n",
    "    \"\"\"\n",
    "    if not dataframes:\n",
    "        return set()\n",
    "\n",
    "    # 将所有 'user' 列的值合并为一个 numpy 数组\n",
    "    all_users = [df['user'].values for df in dataframes if 'user' in df.columns]\n",
    "\n",
    "    # 如果没有有效的 DataFrame，返回空集合\n",
    "    if not all_users:\n",
    "        return set()\n",
    "\n",
    "    # 使用 numpy 的 reduce 求交集\n",
    "    return set(reduce(np.intersect1d, all_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a55c0076-8ff4-4dee-afd3-a65c83eba488",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_list = []\n",
    "for stage in range(config['test_stage']):\n",
    "    test_set = pd.read_csv(saved_data_path + f'test_set_{stage}.csv')\n",
    "    test_set_list.append(test_set)\n",
    "# process_user_interaction_data(test_set, warm_item_list, cold_item_list, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b461487c-a81d-4ac5-9357-786834fd5787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_mse(list1, list2):\n",
    "    \"\"\"\n",
    "    计算两个列表之间的均方误差（MSE）。\n",
    "\n",
    "    参数:\n",
    "    - list1: 第一个列表\n",
    "    - list2: 第二个列表\n",
    "\n",
    "    返回:\n",
    "    - mse: 两个列表之间的均方误差\n",
    "    \"\"\"\n",
    "    return np.mean((np.array(list1) - np.array(list2)) ** 2)\n",
    "\n",
    "def average_mse(array1, array2):\n",
    "    \"\"\"\n",
    "    计算两个数组中一一对应的列表之间的 MSE，并返回归一化后的平均值。\n",
    "\n",
    "    参数:\n",
    "    - array1: 第一个数组，包含五个列表\n",
    "    - array2: 第二个数组，包含五个列表\n",
    "\n",
    "    返回:\n",
    "    - avg_mse: 归一化后的五个 MSE 值的平均值（范围 [0, 1]）\n",
    "    \"\"\"\n",
    "    if len(array1) != len(array2):\n",
    "        raise ValueError(\"两个数组的长度必须相同\")\n",
    "\n",
    "    # 计算每个对应列表的 MSE\n",
    "    mse_values = [calculate_mse(list1, list2) for list1, list2 in zip(array1, array2)]\n",
    "\n",
    "    # 归一化 MSE 值\n",
    "    normalized_mse_values = [mse / 4 for mse in mse_values]\n",
    "\n",
    "    # 计算归一化后的 MSE 平均值\n",
    "    avg_mse = np.mean(normalized_mse_values)\n",
    "    return avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b978db4c-91a7-4d5e-b4cf-bd439e477d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_users = get_common_users(test_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57c9875f-d686-43b8-826a-f6350c776da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17286"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7481d6dc-9472-4f74-8988-bbd48be67c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(saved_data_path + 'common_users.npy', common_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a226dd1b-5d4b-4284-9e4c-cf23d748f2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":finished update warm/cold_list: 6.962107 s\n"
     ]
    }
   ],
   "source": [
    "#calculate ground truth\n",
    "start_time = time.time()\n",
    "mode = 'ground_truth'\n",
    "hist_list = []\n",
    "for stage in range(config['test_stage']):\n",
    "    df = test_set_list[stage]\n",
    "    df = df[df['user'].isin(common_users)]\n",
    "    warm_item_list = warm_item_list_stage[stage]\n",
    "    cold_item_list = cold_item_list_stage[stage]\n",
    "    df_processed = process_user_interaction_data(df, warm_item_list, cold_item_list, config)\n",
    "    df_processed.to_csv(saved_data_path + f'unf_{mode}_stage{stage}')\n",
    "    hist_list.append(df_processed['hist_tgf'].to_list())\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\":finished update warm/cold_list: {elapsed_time:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "077f82fb-1422-4790-a40f-25507d385ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276.33456230163574"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8aa96cf-8b70-45ea-8882-6866f753f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(saved_data_path + 'hist_list_ground_truth.npy', hist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b48e6ca1-f4e3-4343-b4ac-7ab4637863ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_list = np.load(saved_data_path + 'hist_list_ground_truth.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac6510c-ea98-4b17-99b6-8821517380b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['test_stage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "879be8b3-eb36-448e-82ce-f523022148f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17286"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hist_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75b9b4b5-acfe-468e-97aa-43a969e9a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/linghui/drs/daisy_1/steam/lightgcn/metric/\n",
      "/data/linghui/drs/daisy_1/steam/lightgcn/metric/\n",
      "/data/linghui/drs/daisy_1/steam/lightgcn/metric/\n",
      "/data/linghui/drs/daisy_1/steam/lightgcn/metric/\n",
      ":finished update warm/cold_list: 1142.615058 s\n"
     ]
    }
   ],
   "source": [
    "# mf + light_gcn\n",
    "start_time = time.time()\n",
    "for method in ['backbone', 'pd', 'pearson', 'cnif']:\n",
    "    print(saved_metric_path)\n",
    "    config['debias_method'] = method\n",
    "    user_hist_list = []\n",
    "    for stage in range(config['test_stage']):\n",
    "        df = pd.read_csv(saved_rec_path + f\"{config['debias_method']}_{config['dataset']}_{config['algo_name']}_rec_df{stage}\")\n",
    "        df = df[df['user'].isin(common_users)]\n",
    "        warm_item_list = warm_item_list_stage[stage]\n",
    "        cold_item_list = cold_item_list_stage[stage]\n",
    "        df_processed = process_user_interaction_data(df, warm_item_list, cold_item_list, config)\n",
    "        df_processed.to_csv(saved_metric_path + f\"unf_{config['debias_method']}_stage{stage}\")\n",
    "        user_hist_list.append(df_processed['hist_tgf'].to_list()) \n",
    "    np.save(saved_metric_path + f\"hist_list_{config['debias_method']}.npy\", user_hist_list)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\":finished update warm/cold_list: {elapsed_time:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa5cc7a7-a6db-4db4-92a9-fbe691136b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141.8149898052216"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe26e068-71f4-4643-acda-c09a1f21548b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6192"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.user.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ee3f52f-76ac-4bcf-9fc6-51aaca3a69aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 17286)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(user_hist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e748143-483a-4742-bc0d-63b628c0a185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06209650499226749,\n",
       " 0.06728811082521705,\n",
       " 0.0675830702855251,\n",
       " 0.06079082819450367]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list = []\n",
    "for method in ['backbone', 'pd', 'pearson', 'cnif']:\n",
    "    config['debias_method'] = method\n",
    "    user_hist_ground_truth = np.load(saved_data_path + 'hist_list_ground_truth.npy', allow_pickle=True) \n",
    "    user_hist_baseline = np.load(saved_metric_path + f\"hist_list_{config['debias_method']}.npy\", allow_pickle=True) \n",
    "    avg_mse = average_mse(user_hist_ground_truth, user_hist_baseline)\n",
    "    result_list.append(avg_mse)\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8363f019-e9ec-4707-846a-34d922c154f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10197813356697974,\n",
       " 0.12413935504751628,\n",
       " 0.12486927859711269,\n",
       " 0.12311311027588649]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "685c5efa-4483-4f77-9a20-c63fb51026f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":finished update warm/cold_list: 286.855894 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04248475644920431"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mf/lightgcn + fairagent\n",
    "# mf + light_gcn\n",
    "start_time = time.time()\n",
    "user_hist_list = []\n",
    "# rec_results_fair_alpha2.0_beta0.5_gama0.1_stage0.csv\n",
    "for stage in range(config['test_stage']):\n",
    "    df = pd.read_csv(saved_metric_path + f\"fairagent_rec_results_fair_alpha0.0_beta0.0_gama0.1_stage{stage}.csv\")\n",
    "    df = df[df['user'].isin(common_users)]\n",
    "    warm_item_list = warm_item_list_stage[stage]\n",
    "    cold_item_list = cold_item_list_stage[stage]\n",
    "    df_processed = process_user_interaction_data(df, warm_item_list, cold_item_list, config)\n",
    "    df_processed.to_csv(saved_metric_path + f\"fairagent_unf_stage{stage}\")\n",
    "    user_hist_list.append(df_processed['hist_tgf'].to_list()) \n",
    "np.save(saved_metric_path + f\"fairagent_hist_list.npy\", user_hist_list)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\":finished update warm/cold_list: {elapsed_time:.6f} s\")\n",
    "\n",
    "user_hist_ground_truth = np.load(saved_data_path + 'hist_list_ground_truth.npy') \n",
    "user_hist_baseline = np.load(saved_metric_path + f\"fairagent_hist_list.npy\") \n",
    "avg_mse = average_mse(user_hist_ground_truth, user_hist_baseline)\n",
    "avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dde3b6b-65b5-4480-8431-aa99b37b43e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02395332419483007"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cec01c-fb5b-4cd6-a0b2-9c922119bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2971f1e4-2469-4d2e-8d64-15b766948bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/linghui/drs/daisy_1/steam/mf/metric/\n",
      "/data/linghui/drs/daisy_1/steam/mf/metric/\n",
      "/data/linghui/drs/daisy_1/steam/mf/metric/\n",
      "/data/linghui/drs/daisy_1/steam/mf/metric/\n",
      ":finished update warm/cold_list: 1161.540070 s\n"
     ]
    }
   ],
   "source": [
    "#ALDI\n",
    "start_time = time.time()\n",
    "for method in ['backbone', 'pd', 'pearson', 'cnif']:\n",
    "    print(saved_metric_path)\n",
    "    config['debias_method'] = method\n",
    "    user_hist_list = []\n",
    "    for stage in range(config['test_stage']):\n",
    "        # rec_results_aldi_backbone_stage0.csv\n",
    "        df = pd.read_csv(saved_metric_path + f\"rec_results_aldi_{config['debias_method']}_stage{stage}.csv\")\n",
    "        df = df[df['user'].isin(common_users)]\n",
    "        warm_item_list = warm_item_list_stage[stage]\n",
    "        cold_item_list = cold_item_list_stage[stage]\n",
    "        df_processed = process_user_interaction_data(df, warm_item_list, cold_item_list, config)\n",
    "        df_processed.to_csv(saved_metric_path + f\"ALDI_unf_{config['debias_method']}_stage{stage}\")\n",
    "        user_hist_list.append(df_processed['hist_tgf'].to_list()) \n",
    "    np.save(saved_metric_path + f\"ALDI_hist_list_{config['debias_method']}.npy\", user_hist_list)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\":finished update warm/cold_list: {elapsed_time:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9293b8-17fd-43bb-83c8-900fd2955e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 17286)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(user_hist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12d9a2-29f5-41d1-8dc1-fa96a4811df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.045395306345969486,\n",
       " 0.040035236661726564,\n",
       " 0.03392357244545219,\n",
       " 0.025276180560409135]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list = []\n",
    "for method in ['backbone', 'pd', 'pearson', 'cnif']:\n",
    "    config['debias_method'] = method\n",
    "    user_hist_ground_truth = np.load(saved_data_path + 'hist_list_ground_truth.npy') \n",
    "    user_hist_baseline = np.load(saved_metric_path + f\"ALDI_hist_list_{config['debias_method']}.npy\") \n",
    "    avg_mse = average_mse(user_hist_ground_truth, user_hist_baseline)\n",
    "    result_list.append(avg_mse)\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "727b5a1f-36d4-48d3-8d4a-8ed4a88c4c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7018536273569832"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(user_hist_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75f93984-5e7f-4c2d-891e-1927d896e048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33385620218463186"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(user_hist_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "864318bf-57e9-4d17-a708-0367d441dfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":finished update warm/cold_list: 285.194105 s\n"
     ]
    }
   ],
   "source": [
    "#ALDI + fairagent\n",
    "start_time = time.time()\n",
    "user_hist_list = []\n",
    "for stage in range(config['test_stage']):\n",
    "    # rec_results_aldi_backbone_stage0.csv\n",
    "    df = pd.read_csv(saved_metric_path + f\"ALDI_rec_results_fair_alpha2.0_beta0.2_gama0.1_stage{stage}.csv\")\n",
    "    df = df[df['user'].isin(common_users)]\n",
    "    warm_item_list = warm_item_list_stage[stage]\n",
    "    cold_item_list = cold_item_list_stage[stage]\n",
    "    df_processed = process_user_interaction_data(df, warm_item_list, cold_item_list, config)\n",
    "    df_processed.to_csv(saved_metric_path + f\"ALDI_fairagent_unf_stage{stage}\")\n",
    "    user_hist_list.append(df_processed['hist_tgf'].to_list()) \n",
    "np.save(saved_metric_path + f\"ALDI_fairagent_hist_list.npy\", user_hist_list)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\":finished update warm/cold_list: {elapsed_time:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dba6b987-404d-4a79-8225-7efb068d9308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.040556648009419015"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_hist_ground_truth = np.load(saved_data_path + 'hist_list_ground_truth.npy') \n",
    "user_hist_baseline = np.load(saved_metric_path + f\"ALDI_fairagent_hist_list.npy\") \n",
    "avg_mse = average_mse(user_hist_ground_truth, user_hist_baseline)\n",
    "avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1229ca-8df9-4b0c-b78e-4dd2ab33c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALDI_rec_results_fair_alpha2.0_beta0.5_gama0.1_stage0.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
