{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c32af-40f5-42b9-837d-a0ef3200b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy.io as sio\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from logging import getLogger\n",
    "from ast import Global\n",
    "from functools import reduce\n",
    "\n",
    "from ALDI import *\n",
    "from utils import *\n",
    "import daisy\n",
    "from daisy.utils.config import init_seed, init_config, init_logger\n",
    "from daisy.utils.metrics import MAP, NDCG, Recall, Precision, HR, MRR\n",
    "from daisy.utils.utils import get_history_matrix, get_ur, build_candidates_set, ensure_dir, get_inter_matrix\n",
    "from daisy.model.MFRecommender import MF\n",
    "\n",
    "from daisy.model.LightGCNRecommender import LightGCN\n",
    "from daisy.utils.metrics import calc_ranking_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740a01d-e194-4975-b8d8-e65763b8fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'mostpop': MostPop,\n",
    "    'slim': SLiM,\n",
    "    'itemknn': ItemKNNCF,\n",
    "    'puresvd': PureSVD,\n",
    "    'mf': MF,\n",
    "    'fm': FM,\n",
    "    'ngcf': NGCF,\n",
    "    'neumf': NeuMF,\n",
    "    'nfm': NFM,\n",
    "    'multi-vae': VAECF,\n",
    "    'item2vec': Item2Vec,\n",
    "    'ease': EASE,\n",
    "    'lightgcn': LightGCN,\n",
    "}\n",
    "\n",
    "\n",
    "config = init_config()\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "init_logger(config)\n",
    "logger = getLogger()\n",
    "logger.info(config)\n",
    "config['logger'] = logger\n",
    "\n",
    "save_path = config['save_path'] + config['version']\n",
    "ensure_dir(save_path)\n",
    "\n",
    "file_path = save_path + f'{config[\"dataset\"]}/'\n",
    "ensure_dir(file_path)\n",
    "\n",
    "saved_data_path = config['save_path'] + f'daisy_1/{config[\"dataset\"]}/' + 'data/'\n",
    "ensure_dir(saved_data_path)\n",
    "\n",
    "saved_result_path = file_path + f'{config[\"algo_name\"]}/'\n",
    "ensure_dir(saved_result_path)\n",
    "\n",
    "saved_model_path = saved_result_path + 'model/'\n",
    "ensure_dir(saved_model_path)\n",
    "\n",
    "saved_trd_path = saved_result_path + f'{trd_version}/'\n",
    "ensure_dir(saved_trd_path)\n",
    "\n",
    "saved_rec_path = saved_result_path + 'rec_list/'\n",
    "ensure_dir(saved_rec_path)\n",
    "\n",
    "saved_metric_path = saved_result_path + 'metric/'\n",
    "ensure_dir(saved_metric_path)\n",
    "config['res_path'] = saved_metric_path\n",
    "\n",
    "ui_num = np.load(saved_data_path + 'ui_cate.npy')\n",
    "config['user_num'] = ui_num[0]\n",
    "config['item_num'] = ui_num[1]\n",
    "\n",
    "print(f\"user number: {config['user_num']}  item number: {config['item_num']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ffc5716-775c-489f-bfe6-62a3bb4f14f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23 Jan 09:57 INFO - begin test stage 0\n",
      "23 Jan 09:57 INFO - :finished update warm/cold_list: 0.075160 s\n",
      "23 Jan 09:57 INFO - begin test stage 1\n",
      "23 Jan 09:57 INFO - :finished update warm/cold_list: 0.129882 s\n",
      "23 Jan 09:57 INFO - begin test stage 2\n",
      "23 Jan 09:57 INFO - :finished update warm/cold_list: 0.110933 s\n",
      "23 Jan 09:57 INFO - begin test stage 3\n",
      "23 Jan 09:57 INFO - :finished update warm/cold_list: 0.135777 s\n",
      "23 Jan 09:57 INFO - begin test stage 4\n",
      "23 Jan 09:57 INFO - :finished update warm/cold_list: 0.150264 s\n"
     ]
    }
   ],
   "source": [
    "train_total_set = pd.read_csv(saved_data_path + 'train_total_set.csv', index_col=0)\n",
    "test_u_all = np.load(saved_data_path + 'test_u_all.npy', allow_pickle=True)\n",
    "test_ucands_all = np.load(saved_data_path + 'test_ucands_all.npy',allow_pickle=True)\n",
    "test_ur_all = np.load(saved_data_path + 'test_ur_all.npy', allow_pickle=True)\n",
    "data_last_stage = train_total_set\n",
    "# result_df_all = pd.DataFrame()\n",
    "warm_item_list_stage = []\n",
    "cold_item_list_stage = []\n",
    "new_item_list_stage = []\n",
    "for stage in range(config['test_stage']):\n",
    "    logger.info(f'begin test stage {stage}')\n",
    "    # load test data\n",
    "    test_u = test_u_all[stage]\n",
    "    test_ur = test_ur_all[stage]\n",
    "    test_ucands = test_ucands_all[stage]\n",
    "    test_set = pd.read_csv(saved_data_path + f'test_set_{stage}.csv')\n",
    "    # test_pd = pd.read_csv(saved_data_path + f'test_df_{stage}', index_col = 0)\n",
    "\n",
    "    # update warm ï¼Œcold set and new items in this stage\n",
    "    start_time = time.time()\n",
    "    warm_item_list, cold_item_list = update_new_item(data_last_stage, config)\n",
    "    warm_item_list_stage.append(warm_item_list)\n",
    "    cold_item_list_stage.append(cold_item_list)\n",
    "    # config['warm_item_list'] = warm_item_list\n",
    "    # config['cold_item_list'] = cold_item_list\n",
    "    new_item_list = list(set(test_set[config['IID_NAME']].unique()) - set(warm_item_list))\n",
    "    new_item_list_stage.append(new_item_list)\n",
    "    config['topk_list'] = [10, 20, 50]\n",
    "    data_last_stage = pd.concat([data_last_stage, test_set])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logger.info(f\":finished update warm/cold_list: {elapsed_time:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe0872-f4a1-4e51-b1ee-f019773844c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(saved_data_path + 'warm_item_list_stage.npy', warm_item_list_stage)\n",
    "np.save(saved_data_path + 'cold_item_list_stage.npy', cold_item_list_stage)\n",
    "np.save(saved_data_path + 'new_item_list_stage.npy', new_item_list_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b3a2ef-2bcd-467d-9d81-92f810b8fb61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "def set_values_by_id(interaction_history, item_num):\n",
    "    \"\"\"\n",
    "    Convert a user's interaction history into a fixed-length binary list.\n",
    "\n",
    "    Args:\n",
    "        interaction_history (list): List of item IDs that the user has interacted with.\n",
    "        item_num (int): Total number of items.\n",
    "\n",
    "    Returns:\n",
    "        list: A binary list where 1 indicates interaction and 0 indicates no interaction.\n",
    "    \"\"\"\n",
    "    # Use numpy's vectorized operation to check if each item is in the interaction history\n",
    "    return np.isin(np.arange(item_num), interaction_history).astype(int).tolist()\n",
    "\n",
    "\n",
    "def get_weight(warm_item_list, cold_item_list):\n",
    "    \"\"\"\n",
    "    Compute weights for warm and cold items.\n",
    "\n",
    "    Args:\n",
    "        warm_item_list (list): List of warm item indices.\n",
    "        cold_item_list (list): List of cold item indices.\n",
    "\n",
    "    Returns:\n",
    "        warm_weight (np.array): Weights for warm items (decreasing from warm_num to 1).\n",
    "        cold_weight (np.array): Weights for cold items (increasing from 1 to warm_num-like values).\n",
    "    \"\"\"\n",
    "    warm_num = len(warm_item_list)\n",
    "    warm_weight = np.arange(warm_num, 0, -1)  # Weights decrease from warm_num to 1\n",
    "\n",
    "    cold_num = len(cold_item_list)\n",
    "    if cold_num > 1:\n",
    "        # Weights increase from 1 to warm_num-like values\n",
    "        cold_weight = 1 + np.arange(cold_num) * (warm_num - 1) / (cold_num - 1)\n",
    "    else:\n",
    "        cold_weight = np.array([1])  # If only one cold item, weight is 1\n",
    "\n",
    "    return warm_weight, cold_weight\n",
    "\n",
    "\n",
    "def get_user_hist_tgf(exp_list, warm_item_list, cold_item_list, warm_weight, cold_weight):\n",
    "    \"\"\"\n",
    "    Calculate the Top-K Group Fairness (TGF) for a user based on their exposure distribution.\n",
    "\n",
    "    Args:\n",
    "        exp_list (list): List of exposure values for all items.\n",
    "        warm_item_list (list): List of warm item indices.\n",
    "        cold_item_list (list): List of cold item indices.\n",
    "        warm_weight (np.array): Weights for warm items.\n",
    "        cold_weight (np.array): Weights for cold items.\n",
    "\n",
    "    Returns:\n",
    "        float: TGF value, representing the fairness gap between warm and cold items for the user.\n",
    "    \"\"\"\n",
    "    if np.sum(exp_list) == 0:\n",
    "        return 0  # Return 0 if there is no exposure\n",
    "\n",
    "    # Normalize exposure values\n",
    "    exp_list = exp_list / np.sum(exp_list)\n",
    "\n",
    "    # Extract exposure values for warm and cold items\n",
    "    warm_exp_list = exp_list[warm_item_list]\n",
    "    cold_exp_list = exp_list[cold_item_list]\n",
    "\n",
    "    # Compute weighted exposure for warm and cold items\n",
    "    warm_part = np.sum(warm_exp_list * warm_weight) / len(warm_item_list)\n",
    "    cold_part = np.sum(cold_exp_list * cold_weight) / len(cold_item_list)\n",
    "\n",
    "    # Compute TGF as the difference between warm and cold exposure\n",
    "    user_tgf = warm_part - cold_part\n",
    "    if user_tgf < 0:\n",
    "        # Normalize TGF if it is negative\n",
    "        user_tgf = user_tgf / (len(warm_item_list) / len(cold_item_list))\n",
    "    return user_tgf\n",
    "\n",
    "\n",
    "def calculate_nc(input_set, target_set):\n",
    "    \"\"\"\n",
    "    Calculate the proportion of values in the input set that appear in the target set.\n",
    "\n",
    "    Args:\n",
    "        input_set (list): Input list of values.\n",
    "        target_set (list): Target list of values.\n",
    "\n",
    "    Returns:\n",
    "        float: Proportion of values in the input set that appear in the target set.\n",
    "    \"\"\"\n",
    "    if not input_set:\n",
    "        return 0.0  # Return 0 if the input set is empty\n",
    "    return len(set(input_set) & set(target_set)) / len(input_set)\n",
    "\n",
    "\n",
    "def process_user_interaction_data(train_rl_set, warm_item_list, cold_item_list, config):\n",
    "    \"\"\"\n",
    "    Process user interaction data to generate a DataFrame with user history, exposure lists, TGF, and NC.\n",
    "\n",
    "    Args:\n",
    "        train_rl_set (pd.DataFrame): Training set containing user-item interactions.\n",
    "        warm_item_list (list): List of warm item indices.\n",
    "        cold_item_list (list): List of cold item indices.\n",
    "        config (dict): Configuration dictionary containing 'item_num'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with user history, exposure lists, TGF, and NC.\n",
    "    \"\"\"\n",
    "    # Group by user and generate user history lists\n",
    "    user_rl_df = train_rl_set.groupby('user')['item'].apply(list).reset_index(name='user_history')\n",
    "\n",
    "    # Generate exposure lists for each user\n",
    "    item_range = np.arange(config['item_num'])\n",
    "    user_rl_df['exp_list'] = user_rl_df['user_history'].apply(\n",
    "        lambda x: np.isin(item_range, x).astype(int).tolist()\n",
    "    )\n",
    "\n",
    "    # Compute weights for warm and cold items\n",
    "    warm_weight, cold_weight = get_weight(warm_item_list, cold_item_list)\n",
    "\n",
    "    # Calculate TGF for each user\n",
    "    user_rl_df['hist_tgf'] = user_rl_df['exp_list'].apply(\n",
    "        lambda x: get_user_hist_tgf(np.array(x), warm_item_list, cold_item_list, warm_weight, cold_weight)\n",
    "    )\n",
    "\n",
    "    # Calculate NC for each user\n",
    "    user_rl_df['hist_nc'] = user_rl_df['user_history'].apply(\n",
    "        lambda x: calculate_nc(x, cold_item_list)\n",
    "    )\n",
    "\n",
    "    return user_rl_df\n",
    "\n",
    "\n",
    "def get_common_users(dataframes):\n",
    "    \"\"\"\n",
    "    Extract common 'user' values from multiple DataFrames.\n",
    "\n",
    "    Args:\n",
    "        dataframes (list): List of DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        set: Set of common user IDs.\n",
    "    \"\"\"\n",
    "    if not dataframes:\n",
    "        return set()\n",
    "\n",
    "    # Extract 'user' columns from all DataFrames\n",
    "    all_users = [df['user'].values for df in dataframes if 'user' in df.columns]\n",
    "\n",
    "    if not all_users:\n",
    "        return set()\n",
    "\n",
    "    # Use numpy's reduce to find the intersection of all user arrays\n",
    "    return set(reduce(np.intersect1d, all_users))\n",
    "\n",
    "\n",
    "def calculate_mse(list1, list2):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) between two lists.\n",
    "\n",
    "    Args:\n",
    "        list1 (list): First list.\n",
    "        list2 (list): Second list.\n",
    "\n",
    "    Returns:\n",
    "        float: MSE between the two lists.\n",
    "    \"\"\"\n",
    "    return np.mean((np.array(list1) - np.array(list2)) ** 2)\n",
    "\n",
    "\n",
    "def average_mse(array1, array2):\n",
    "    \"\"\"\n",
    "    Calculate the average MSE between corresponding lists in two arrays, normalized to [0, 1].\n",
    "\n",
    "    Args:\n",
    "        array1 (list): First array containing five lists.\n",
    "        array2 (list): Second array containing five lists.\n",
    "\n",
    "    Returns:\n",
    "        float: Normalized average MSE.\n",
    "    \"\"\"\n",
    "    if len(array1) != len(array2):\n",
    "        raise ValueError(\"Both arrays must have the same length\")\n",
    "\n",
    "    # Calculate MSE for each pair of corresponding lists\n",
    "    mse_values = [calculate_mse(list1, list2) for list1, list2 in zip(array1, array2)]\n",
    "\n",
    "    # Normalize MSE values to [0, 1]\n",
    "    normalized_mse_values = [mse / 4 for mse in mse_values]\n",
    "\n",
    "    # Calculate the average normalized MSE\n",
    "    avg_mse = np.mean(normalized_mse_values)\n",
    "    return avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a55c0076-8ff4-4dee-afd3-a65c83eba488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set - Ground-truth\n",
    "test_set_list = []\n",
    "for stage in range(config['test_stage']):\n",
    "    test_set = pd.read_csv(saved_data_path + f'test_set_{stage}.csv')\n",
    "    test_set_list.append(test_set)\n",
    "# process_user_interaction_data(test_set, warm_item_list, cold_item_list, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b978db4c-91a7-4d5e-b4cf-bd439e477d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get common users for all test stages\n",
    "common_users = get_common_users(test_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7481d6dc-9472-4f74-8988-bbd48be67c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(saved_data_path + 'common_users.npy', common_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a226dd1b-5d4b-4284-9e4c-cf23d748f2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":finished update warm/cold_list: 6.962107 s\n"
     ]
    }
   ],
   "source": [
    "#calculate ground truth - TGF(Hu)\n",
    "start_time = time.time()\n",
    "mode = 'ground_truth'\n",
    "hist_list = []\n",
    "for stage in range(config['test_stage']):\n",
    "    df = test_set_list[stage]\n",
    "    df = df[df['user'].isin(common_users)]\n",
    "    warm_item_list = warm_item_list_stage[stage]\n",
    "    cold_item_list = cold_item_list_stage[stage]\n",
    "    df_processed = process_user_interaction_data(df, warm_item_list, cold_item_list, config)\n",
    "    df_processed.to_csv(saved_data_path + f'unf_{mode}_stage{stage}')\n",
    "    hist_list.append(df_processed['hist_tgf'].to_list())\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\":finished calculate TGF(Hu) for {config['dataset']}: {elapsed_time:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8aa96cf-8b70-45ea-8882-6866f753f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(saved_data_path + 'hist_list_ground_truth.npy', hist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9b4b5-acfe-468e-97aa-43a969e9a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TGF(Lu) for backbone models/baseline.\n",
    "start_time = time.time()\n",
    "for method in ['backbone', 'pd', 'pearson', 'cnif']:\n",
    "    print(saved_metric_path)\n",
    "    config['debias_method'] = method\n",
    "    user_hist_list = []\n",
    "    for stage in range(config['test_stage']):\n",
    "        # Top-K recommendation results of backbone models/baseline\n",
    "        df = pd.read_csv(saved_rec_path + f\"{config['debias_method']}_{config['dataset']}_{config['algo_name']}_rec_df{stage}\")\n",
    "        df = df[df['user'].isin(common_users)]\n",
    "        warm_item_list = warm_item_list_stage[stage]\n",
    "        cold_item_list = cold_item_list_stage[stage]\n",
    "        df_processed = process_user_interaction_data(df, warm_item_list, cold_item_list, config)\n",
    "        df_processed.to_csv(saved_metric_path + f\"unf_{config['debias_method']}_stage{stage}\")\n",
    "        user_hist_list.append(df_processed['hist_tgf'].to_list()) \n",
    "    np.save(saved_metric_path + f\"hist_list_{config['debias_method']}.npy\", user_hist_list)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\":finished calculate TGF(Lu) for {config['debias_method']}: {elapsed_time:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e748143-483a-4742-bc0d-63b628c0a185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06209650499226749,\n",
       " 0.06728811082521705,\n",
       " 0.0675830702855251,\n",
       " 0.06079082819450367]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list = []\n",
    "for method in ['backbone', 'pd', 'pearson', 'cnif']:\n",
    "    config['debias_method'] = method\n",
    "    user_hist_ground_truth = np.load(saved_data_path + 'hist_list_ground_truth.npy', allow_pickle=True) \n",
    "    user_hist_baseline = np.load(saved_metric_path + f\"hist_list_{config['debias_method']}.npy\", allow_pickle=True) \n",
    "    avg_mse = average_mse(user_hist_ground_truth, user_hist_baseline)\n",
    "    result_list.append(avg_mse)\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c5efa-4483-4f77-9a20-c63fb51026f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TGF(Lu) for FairAgent.\n",
    "start_time = time.time()\n",
    "start_time = time.time()\n",
    "user_hist_list = []\n",
    "for stage in range(config['test_stage']):\n",
    "    # Top-K recommendation results of FairAgent\n",
    "    df = pd.read_csv(saved_metric_path + f\"fairagent_rec_results_fair_alpha0.0_beta0.0_gama0.1_stage{stage}.csv\")\n",
    "    df = df[df['user'].isin(common_users)]\n",
    "    warm_item_list = warm_item_list_stage[stage]\n",
    "    cold_item_list = cold_item_list_stage[stage]\n",
    "    df_processed = process_user_interaction_data(df, warm_item_list, cold_item_list, config)\n",
    "    df_processed.to_csv(saved_metric_path + f\"fairagent_unf_stage{stage}\")\n",
    "    user_hist_list.append(df_processed['hist_tgf'].to_list()) \n",
    "np.save(saved_metric_path + f\"fairagent_hist_list.npy\", user_hist_list)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\":finished calculate TGF(Lu) for FairAget: {elapsed_time:.6f} s\")\n",
    "\n",
    "user_hist_ground_truth = np.load(saved_data_path + 'hist_list_ground_truth.npy') \n",
    "user_hist_baseline = np.load(saved_metric_path + f\"fairagent_hist_list.npy\") \n",
    "avg_mse = average_mse(user_hist_ground_truth, user_hist_baseline)\n",
    "avg_mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
